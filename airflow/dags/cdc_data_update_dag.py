#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­å›½ç–¾æ§ä¸­å¿ƒç›‘æµ‹æ•°æ®è‡ªåŠ¨æ›´æ–° DAG
æ¯å‘¨å®šæ—¶æ‰§è¡Œæ•°æ®æŠ“å–ã€è½¬æ¢ã€æå–å’Œåˆå¹¶æµç¨‹
"""

from datetime import datetime, timedelta
from pathlib import Path
import os
import sys
import logging
from typing import List, Set
import requests
from bs4 import BeautifulSoup

from airflow import DAG
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable

# é»˜è®¤é…ç½®
DEFAULT_ARGS = {
    'owner': 'cdc_data_team',
    'depends_on_past': False,
    'email': ['yang_kaixin@gzlab.ac.cn'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

# ä» Airflow Variables è·å–é…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
PROJECT_ROOT = Variable.get("cdc_project_root", "/data/ykx/covid19/get_data/cn_cdc_data")
PDF_SERVER = Variable.get("cdc_pdf_server", "http://10.22.16.132:8011")
URL_FILE = Variable.get("cdc_url_file", "config/url_surveillance_new.txt")
HISTORY_URL_FILE = Variable.get("cdc_history_url_file", "config/url_surveillance_history.txt")
CDC_MONITORING_URL = Variable.get("cdc_monitoring_url", "https://www.chinacdc.cn/jksj/jksj04_14275/")
FIRECRAWL_API_KEY = Variable.get("firecrawl_api_key", "fc-1070b8415da9472c87f06cfd01cf0ecf")
UPDATE_DIR = Variable.get("cdc_update_dir", "update")
ALL_CSV = Variable.get("cdc_all_csv", "data/cncdc_surveillance_all.csv")
COVID_CSV = Variable.get("cdc_covid_csv", "data/cncdc_surveillance_covid19.csv")


# ==================== çˆ¬è™«è¾…åŠ©å‡½æ•° ====================

def fetch_surveillance_links_firecrawl(url: str, api_key: str) -> List[str]:
    """
    ä½¿ç”¨ Firecrawl API v2 çˆ¬å–é“¾æ¥
    
    Args:
        url: ç›®æ ‡URL
        api_key: Firecrawl APIå¯†é’¥
        
    Returns:
        é“¾æ¥åˆ—è¡¨
    """
    # Firecrawl v2 API - ä½¿ç”¨ map æ¥å£è·å–ç½‘ç«™æ‰€æœ‰é“¾æ¥
    api_url = "https://api.firecrawl.dev/v2/map"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "url": url,
        "limit": 5000,
        "includeSubdomains": True,
        "sitemap": "include"
    }
    
    try:
        print(f"ğŸ”¥ ä½¿ç”¨ Firecrawl API v2 çˆ¬å–: {url}")
        print(f"â±ï¸  è¯·æ±‚è¶…æ—¶è®¾ç½®: 90ç§’")
        
        response = requests.post(api_url, json=payload, headers=headers, timeout=90)
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code == 408:
            print(f"âš ï¸  Firecrawl è¶…æ—¶ (408)ï¼ŒCDCç½‘ç«™å“åº”è¾ƒæ…¢")
            return []
        
        response.raise_for_status()
        data = response.json()
        
        # v2 API å“åº”æ ¼å¼: { "success": true, "links": [...] }
        if data.get('success'):
            all_links = data.get('links', [])
            
            if isinstance(all_links, list):
                print(f"âœ… Firecrawl è¿”å› {len(all_links)} ä¸ªé“¾æ¥")
                
                # è¿‡æ»¤å‡ºç›‘æµ‹æŠ¥å‘Šé“¾æ¥
                links = []
                for link in all_links:
                    # v2 API links å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è±¡
                    if isinstance(link, str):
                        link_url = link
                    elif isinstance(link, dict):
                        link_url = link.get('url', '')
                    else:
                        continue
                    
                    # åŒ¹é…ç›‘æµ‹æŠ¥å‘Šé“¾æ¥æ¨¡å¼
                    # æ’é™¤ç´¢å¼•é¡µé¢ï¼ˆåŒ…å« index_ï¼‰
                    if ('/jksj/jksj04_14275/' in link_url and 
                        link_url.endswith('.html') and 
                        'index_' not in link_url):
                        # ç¡®ä¿æ˜¯å®Œæ•´URL
                        if not link_url.startswith('http'):
                            link_url = 'https://www.chinacdc.cn' + link_url
                        links.append(link_url)
                
                # å»é‡å¹¶æ’åº
                links = sorted(list(set(links)))
                print(f"âœ… è¿‡æ»¤åå¾—åˆ° {len(links)} ä¸ªç›‘æµ‹æŠ¥å‘Šé“¾æ¥")
                
                if len(links) > 0:
                    print(f"ğŸ“‹ ç¤ºä¾‹é“¾æ¥ï¼ˆå‰3ä¸ªï¼‰:")
                    for link in links[:3]:
                        print(f"  - {link}")
                    return links
                else:
                    print(f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…çš„ç›‘æµ‹æŠ¥å‘Šé“¾æ¥")
                    return []
            else:
                print(f"âš ï¸  Firecrawl è¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {type(all_links)}")
                return []
        else:
            error_msg = data.get('error', data.get('message', 'Unknown error'))
            logging.error(f"Firecrawl API è¿”å›å¤±è´¥: {error_msg}")
            return []
            
    except requests.exceptions.Timeout:
        logging.warning("Firecrawl API è¯·æ±‚è¶…æ—¶ï¼ˆ90ç§’ï¼‰ï¼ŒCDCç½‘ç«™å¯èƒ½å“åº”è¾ƒæ…¢")
        return []
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 408:
            logging.warning("Firecrawl æœåŠ¡è¶…æ—¶ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼ˆCDCç½‘ç«™å“åº”æ…¢ï¼‰")
        else:
            logging.error(f"Firecrawl API HTTPé”™è¯¯: {e.response.status_code} - {e}")
        return []
    except Exception as e:
        logging.error(f"Firecrawl API è°ƒç”¨å¤±è´¥: {e}")
        # ä¸æ‰“å°å®Œæ•´å †æ ˆï¼Œé¿å…æ—¥å¿—è¿‡é•¿
        return []


def fetch_surveillance_links_bs4(url: str) -> List[str]:
    """
    ä½¿ç”¨ BeautifulSoup å¤‡ç”¨æ–¹æ¡ˆçˆ¬å–é“¾æ¥
    
    Args:
        url: ç›‘æµ‹æ•°æ®é¡µé¢URL
        
    Returns:
        é“¾æ¥åˆ—è¡¨
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    
    try:
        print(f"ğŸŒ ä½¿ç”¨ BeautifulSoup å¤‡ç”¨æ–¹æ¡ˆçˆ¬å–: {url}")
        response = requests.get(url, headers=headers, timeout=30, verify=True)
        response.encoding = 'utf-8'
        response.raise_for_status()
        
        print(f"âœ… å“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"âœ… å“åº”å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„é“¾æ¥
        all_links = soup.find_all('a', href=True)
        print(f"ğŸ” é¡µé¢ä¸­æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥æ ‡ç­¾")
        
        # æŸ¥æ‰¾ç›‘æµ‹æŠ¥å‘Šé“¾æ¥
        links = []
        for a_tag in all_links:
            href = a_tag['href']
            
            # åŒ¹é…ç›‘æµ‹æŠ¥å‘Šé“¾æ¥æ¨¡å¼
            # æ’é™¤ç´¢å¼•é¡µé¢ï¼ˆåŒ…å« index_ï¼‰
            if ('/jksj/jksj04_14275/' in href and 
                href.endswith('.html') and 
                'index_' not in href):
                # è½¬æ¢ä¸ºå®Œæ•´URL
                if href.startswith('http'):
                    full_url = href
                else:
                    base_url = 'https://www.chinacdc.cn'
                    full_url = base_url + href if href.startswith('/') else base_url + '/' + href
                
                links.append(full_url)
        
        # å»é‡å¹¶æ’åº
        links = sorted(list(set(links)))
        
        print(f"âœ… BeautifulSoup çˆ¬å–åˆ° {len(links)} ä¸ªç›‘æµ‹æŠ¥å‘Šé“¾æ¥")
        
        if len(links) > 0:
            print(f"ğŸ“‹ ç¤ºä¾‹é“¾æ¥ï¼ˆå‰3ä¸ªï¼‰:")
            for link in links[:3]:
                print(f"  - {link}")
        
        return links
        
    except Exception as e:
        logging.error(f"BeautifulSoup çˆ¬å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def fetch_surveillance_links(url: str = "https://www.chinacdc.cn/jksj/jksj04_14275/") -> List[str]:
    """
    ä»ä¸­å›½ç–¾æ§ä¸­å¿ƒç½‘ç«™çˆ¬å–å“¨ç‚¹ç›‘æµ‹æŠ¥å‘Šé“¾æ¥
    ä¼˜å…ˆä½¿ç”¨ Firecrawl APIï¼Œå¤±è´¥æ—¶å›é€€åˆ° BeautifulSoup
    
    Args:
        url: ç›‘æµ‹æ•°æ®é¡µé¢URL
        
    Returns:
        é“¾æ¥åˆ—è¡¨
    """
    # æ–¹æ³•1: å°è¯•ä½¿ç”¨ Firecrawl API
    if FIRECRAWL_API_KEY and FIRECRAWL_API_KEY != "":
        links = fetch_surveillance_links_firecrawl(url, FIRECRAWL_API_KEY)
        if links:
            return links
        logging.warning("Firecrawl API å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
    
    # æ–¹æ³•2: å¤‡ç”¨æ–¹æ¡ˆ - BeautifulSoup
    links = fetch_surveillance_links_bs4(url)
    if links:
        return links
    
    logging.error("æ‰€æœ‰çˆ¬å–æ–¹æ³•éƒ½å¤±è´¥äº†")
    return []


def read_existing_links(file_path: str) -> Set[str]:
    """
    è¯»å–æœ¬åœ°å·²è®°å½•çš„é“¾æ¥
    
    Args:
        file_path: é“¾æ¥æ–‡ä»¶è·¯å¾„
        
    Returns:
        é“¾æ¥é›†åˆ
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        print(f"âš ï¸  é“¾æ¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return set()
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            links = set(line.strip() for line in f if line.strip() and line.strip().startswith('http'))
        
        print(f"âœ… è¯»å–åˆ° {len(links)} ä¸ªå·²æœ‰é“¾æ¥")
        return links
                
    except Exception as e:
        logging.error(f"è¯»å–é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
        return set()


def find_new_links(fetched_links: List[str], existing_links: Set[str]) -> List[str]:
    """
    æ‰¾å‡ºæ–°çš„é“¾æ¥
    
    Args:
        fetched_links: çˆ¬å–åˆ°çš„é“¾æ¥åˆ—è¡¨
        existing_links: æœ¬åœ°å·²æœ‰çš„é“¾æ¥é›†åˆ
        
    Returns:
        æ–°é“¾æ¥åˆ—è¡¨ï¼ˆæŒ‰æ—¥æœŸæ’åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼‰
    """
    new_links = [link for link in fetched_links if link not in existing_links]
    
    # æŒ‰URLä¸­çš„æ—¥æœŸæ’åºï¼ˆé™åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼‰
    new_links.sort(reverse=True)
    
    print(f"âœ… å‘ç° {len(new_links)} ä¸ªæ–°é“¾æ¥")
    return new_links


def append_links_to_file(file_path: str, new_links: List[str]):
    """
    å°†æ–°é“¾æ¥è¿½åŠ åˆ°æ–‡ä»¶
    
    Args:
        file_path: é“¾æ¥æ–‡ä»¶è·¯å¾„
        new_links: æ–°é“¾æ¥åˆ—è¡¨
    """
    if not new_links:
        print("âš ï¸  æ²¡æœ‰æ–°é“¾æ¥éœ€è¦å†™å…¥")
        return
    
    file_path = Path(file_path)
    
    try:
        # è¯»å–ç°æœ‰å†…å®¹
        existing_content = []
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                existing_content = [line.rstrip('\n') for line in f]
        
        # ç§»é™¤æœ«å°¾çš„ç©ºè¡Œ
        while existing_content and not existing_content[-1]:
            existing_content.pop()
        
        # è¿½åŠ æ–°é“¾æ¥
        with open(file_path, 'w', encoding='utf-8') as f:
            # å†™å…¥ç°æœ‰å†…å®¹
            for line in existing_content:
                f.write(line + '\n')
            
            # å†™å…¥æ–°é“¾æ¥
            for link in new_links:
                f.write(link + '\n')
        
        print(f"âœ… å·²å°† {len(new_links)} ä¸ªæ–°é“¾æ¥å†™å…¥æ–‡ä»¶: {file_path}")
                
    except Exception as e:
        logging.error(f"å†™å…¥é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
        raise


# ==================== DAG ä»»åŠ¡å‡½æ•° ====================

def fetch_new_links_task(**context):
    """
    çˆ¬å–æ–°çš„ç›‘æµ‹æŠ¥å‘Šé“¾æ¥
    ä» CDC ç½‘ç«™çˆ¬å–é“¾æ¥ï¼Œä¸æœ¬åœ°å†å²è®°å½•å¯¹æ¯”ï¼Œæ‰¾å‡ºæ–°é“¾æ¥
    """
    import sys
    sys.path.insert(0, PROJECT_ROOT)
    
    print(f"ğŸŒ å¼€å§‹çˆ¬å–ç›‘æµ‹æŠ¥å‘Šé“¾æ¥: {CDC_MONITORING_URL}")
    
    # çˆ¬å–é“¾æ¥
    fetched_links = fetch_surveillance_links(CDC_MONITORING_URL)
    
    # è¯»å–æœ¬åœ°å†å²è®°å½•
    history_file_path = Path(PROJECT_ROOT) / HISTORY_URL_FILE
    existing_links = read_existing_links(str(history_file_path))
    
    if not fetched_links:
        print("âš ï¸  æœªèƒ½ä»ç½‘ç«™çˆ¬å–åˆ°é“¾æ¥")
        
        # å¦‚æœå†å²è®°å½•å­˜åœ¨ï¼Œä¸ç®—é”™è¯¯ï¼Œåªæ˜¯æ²¡æœ‰æ–°é“¾æ¥
        if existing_links:
            print(f"âœ… ä½¿ç”¨å†å²è®°å½•: {len(existing_links)} ä¸ªå·²çŸ¥é“¾æ¥")
            print("âœ… æœ¬æ¬¡è¿è¡Œï¼šæ²¡æœ‰æ–°é“¾æ¥éœ€è¦å¤„ç†")
            context['task_instance'].xcom_push(key='new_links', value=[])
            context['task_instance'].xcom_push(key='link_count', value=0)
            context['task_instance'].xcom_push(key='has_new_links', value=False)
            context['task_instance'].xcom_push(key='crawl_failed', value=True)
            return 0
        else:
            # å†å²è®°å½•ä¹Ÿä¸å­˜åœ¨ï¼Œå°è¯•ä»å¤‡ç”¨æ–‡ä»¶è¯»å–
            print("âš ï¸  å†å²è®°å½•ä¸å­˜åœ¨ï¼Œå°è¯•ä»å¤‡ç”¨é…ç½®æ–‡ä»¶è¯»å–")
            url_file_path = Path(PROJECT_ROOT) / URL_FILE
            if url_file_path.exists():
                with open(url_file_path, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                if urls:
                    print(f"âœ… ä»å¤‡ç”¨æ–‡ä»¶è¯»å–åˆ° {len(urls)} ä¸ªé“¾æ¥")
                    context['task_instance'].xcom_push(key='new_links', value=urls)
                    context['task_instance'].xcom_push(key='link_count', value=len(urls))
                    context['task_instance'].xcom_push(key='has_new_links', value=True)
                    context['task_instance'].xcom_push(key='crawl_failed', value=True)
                    return len(urls)
                    
                    # æ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥
                    logging.error("æ— æ³•è·å–ä»»ä½•é“¾æ¥ï¼ˆçˆ¬å–å¤±è´¥ã€å†å²è®°å½•ä¸å­˜åœ¨ã€å¤‡ç”¨æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼‰")
                    logging.info("ğŸ’¡ å»ºè®®: è¯·æ‰‹åŠ¨åœ¨ config/url_surveillance_new.txt ä¸­æ·»åŠ é“¾æ¥ï¼Œæˆ–ç­‰å¾…ç½‘ç«™æ¢å¤")
                    context['task_instance'].xcom_push(key='new_links', value=[])
                    context['task_instance'].xcom_push(key='link_count', value=0)
                    context['task_instance'].xcom_push(key='has_new_links', value=False)
                    context['task_instance'].xcom_push(key='crawl_failed', value=True)
                    return 0
    
    # æˆåŠŸçˆ¬å–åˆ°é“¾æ¥
    print(f"âœ… ä»ç½‘ç«™æˆåŠŸçˆ¬å– {len(fetched_links)} ä¸ªé“¾æ¥")
    
    # æ‰¾å‡ºæ–°é“¾æ¥
    new_links = find_new_links(fetched_links, existing_links)
    
    if not new_links:
        print("âš ï¸  æ²¡æœ‰å‘ç°æ–°é“¾æ¥ï¼ˆæ‰€æœ‰é“¾æ¥éƒ½å·²åœ¨å†å²è®°å½•ä¸­ï¼‰")
        context['task_instance'].xcom_push(key='new_links', value=[])
        context['task_instance'].xcom_push(key='link_count', value=0)
        context['task_instance'].xcom_push(key='has_new_links', value=False)
        context['task_instance'].xcom_push(key='crawl_failed', value=False)
        return 0
    
    print(f"âœ… å‘ç° {len(new_links)} ä¸ªæ–°é“¾æ¥:")
    for link in new_links[:10]:  # åªæ‰“å°å‰10ä¸ª
        print(f"  - {link}")
    if len(new_links) > 10:
        print(f"  ... è¿˜æœ‰ {len(new_links) - 10} ä¸ªé“¾æ¥")
    
    # å°†æ–°é“¾æ¥æ¨é€åˆ° XCom
    context['task_instance'].xcom_push(key='new_links', value=new_links)
    context['task_instance'].xcom_push(key='link_count', value=len(new_links))
    context['task_instance'].xcom_push(key='has_new_links', value=True)
    context['task_instance'].xcom_push(key='crawl_failed', value=False)
    
    return len(new_links)


def check_new_links(**context):
    """
    æ£€æŸ¥æ˜¯å¦æœ‰æ–°é“¾æ¥éœ€è¦å¤„ç†
    å¦‚æœæ²¡æœ‰æ–°é“¾æ¥ï¼Œæå‰ç»“æŸæµç¨‹ï¼ˆè¿”å› Falseï¼‰
    å¦‚æœæœ‰æ–°é“¾æ¥ï¼Œç»§ç»­å¤„ç†ï¼ˆè¿”å› Trueï¼‰
    """
    has_new_links = context['task_instance'].xcom_pull(
        key='has_new_links', 
        task_ids='fetch_new_links'
    )
    
    link_count = context['task_instance'].xcom_pull(
        key='link_count', 
        task_ids='fetch_new_links'
    )
    
    if not has_new_links or link_count == 0:
        print("âš ï¸  æ²¡æœ‰æ–°é“¾æ¥éœ€è¦å¤„ç†ï¼Œç»ˆæ­¢åç»­æµç¨‹")
        return False  # è¿”å› False å°†è·³è¿‡æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡
    
    print(f"âœ… æœ‰ {link_count} ä¸ªæ–°é“¾æ¥éœ€è¦å¤„ç†ï¼Œç»§ç»­æ‰§è¡Œæµç¨‹")
    
    # å°†æ–°é“¾æ¥ä¼ é€’ç»™åç»­ä»»åŠ¡
    new_links = context['task_instance'].xcom_pull(
        key='new_links', 
        task_ids='fetch_new_links'
    )
    
    context['task_instance'].xcom_push(key='url_list', value=new_links)
    context['task_instance'].xcom_push(key='url_count', value=link_count)
    
    return True  # è¿”å› True ç»§ç»­æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡


def download_web_to_pdf(**context):
    """ä¸‹è½½ç½‘é¡µå¹¶ä¿å­˜ä¸º PDF"""
    import sys
    import subprocess
    sys.path.insert(0, PROJECT_ROOT)
    
    os.chdir(PROJECT_ROOT)
    
    urls = context['task_instance'].xcom_pull(key='url_list', task_ids='check_new_links')
    temp_dir = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp" / "pdf"
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {len(urls)} ä¸ªç½‘é¡µä¸º PDF...")
    
    # ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨ï¼ˆAirflow çš„ Pythonï¼‰
    python_exe = sys.executable
    
    for idx, url in enumerate(urls, 1):
        print(f"\nå¤„ç† {idx}/{len(urls)}: {url}")
        
        cmd = [
            python_exe, "src/save_web_to_pdf.py",
            url,
            "-o", str(temp_dir),
            "--format", "A1",
            "--margin", "10mm",
            "-c", "6",
            "--wait-until", "load"
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(result.stdout)
            print(f"âœ… ä¸‹è½½æˆåŠŸ: {url}")
        except subprocess.CalledProcessError as e:
            logging.error(f"ä¸‹è½½å¤±è´¥: {url}")
            logging.error(f"é”™è¯¯ä¿¡æ¯: {e.stderr}")
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª URLï¼Œä¸ä¸­æ–­æ•´ä¸ªä»»åŠ¡
            continue
    
    print(f"\nâœ… PDF ä¸‹è½½ä»»åŠ¡å®Œæˆ")
    return str(temp_dir)


def convert_pdf_to_md(**context):
    """å°† PDF è½¬æ¢ä¸º Markdown"""
    import sys
    import subprocess
    sys.path.insert(0, PROJECT_ROOT)
    
    os.chdir(PROJECT_ROOT)
    
    pdf_dir = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp" / "pdf"
    md_dir = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp" / "md"
    md_dir.mkdir(parents=True, exist_ok=True)
    
    pdf_files = list(pdf_dir.glob("*.pdf"))
    
    if not pdf_files:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        return str(md_dir)
    
    print(f"ğŸ“ å¼€å§‹è½¬æ¢ {len(pdf_files)} ä¸ª PDF æ–‡ä»¶ä¸º Markdown...")
    
    # ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨ï¼ˆAirflow çš„ Pythonï¼‰
    python_exe = sys.executable
    
    for pdf_file in pdf_files:
        print(f"\nè½¬æ¢: {pdf_file.name}")
        
        cmd = [
            python_exe, "src/convert_pdf_to_md.py",
            str(pdf_file),
            "-o", str(md_dir),
            "--server", PDF_SERVER,
            "--lang", "ch",
            "--backend", "pipeline",
            "--parse-method", "auto",
            "--formula-enable", "true",
            "--table-enable", "true",
            "--workers", "6",
            "--timeout", "180"
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(result.stdout)
            print(f"âœ… è½¬æ¢æˆåŠŸ: {pdf_file.name}")
        except subprocess.CalledProcessError as e:
            logging.error(f"è½¬æ¢å¤±è´¥: {pdf_file.name}")
            logging.error(f"é”™è¯¯ä¿¡æ¯: {e.stderr}")
            continue
    
    print(f"\nâœ… PDF è½¬æ¢ä»»åŠ¡å®Œæˆ")
    return str(md_dir)


def extract_data_from_md(**context):
    """ä» Markdown æå–ç»“æ„åŒ–æ•°æ®"""
    import sys
    import subprocess
    sys.path.insert(0, PROJECT_ROOT)
    
    os.chdir(PROJECT_ROOT)
    
    md_dir = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp" / "md"
    csv_dir = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp" / "csv"
    csv_dir.mkdir(parents=True, exist_ok=True)
    
    md_files = list(md_dir.glob("*.md"))
    
    if not md_files:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° Markdown æ–‡ä»¶")
        return str(csv_dir)
    
    print(f"ğŸ“Š å¼€å§‹ä» {len(md_files)} ä¸ª Markdown æ–‡ä»¶æå–æ•°æ®...")
    
    # ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨ï¼ˆAirflow çš„ Pythonï¼‰
    python_exe = sys.executable
    
    for md_file in md_files:
        csv_file = csv_dir / f"{md_file.stem}.csv"
        print(f"\næå–: {md_file.name}")
        
        cmd = [
            python_exe, "src/extract_surveillance_data.py",
            str(md_file),
            "-o", str(csv_file)
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(result.stdout)
            print(f"âœ… æå–æˆåŠŸ: {md_file.name}")
        except subprocess.CalledProcessError as e:
            logging.error(f"æå–å¤±è´¥: {md_file.name}")
            logging.error(f"é”™è¯¯ä¿¡æ¯: {e.stderr}")
            continue
    
    print(f"\nâœ… æ•°æ®æå–ä»»åŠ¡å®Œæˆ")
    return str(csv_dir)


def organize_and_merge_data(**context):
    """æ•´ç†æ•°æ®åˆ°ç›®æ ‡ç›®å½•å¹¶åˆå¹¶åˆ°ä¸»æ–‡ä»¶"""
    import sys
    import shutil
    import re
    import pandas as pd
    sys.path.insert(0, PROJECT_ROOT)
    
    os.chdir(PROJECT_ROOT)
    
    csv_dir = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp" / "csv"
    pdf_dir = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp" / "pdf"
    md_dir = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp" / "md"
    
    csv_files = list(csv_dir.glob("*.csv"))
    
    if not csv_files:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° CSV æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œæ•°æ®åˆå¹¶")
        return
    
    print(f"ğŸ—‚ï¸ å¼€å§‹æ•´ç†å’Œåˆå¹¶ {len(csv_files)} ä¸ªæ•°æ®æ–‡ä»¶...")
    
    all_csv_path = Path(PROJECT_ROOT) / ALL_CSV
    covid_csv_path = Path(PROJECT_ROOT) / COVID_CSV
    
    for csv_file in csv_files:
        file_id = csv_file.stem  # ä¾‹å¦‚: t20251015_312973
        
        # è¯»å– CSV æå– reference_date
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            if 'reference_date' in df.columns and len(df) > 0:
                ref_date = str(df['reference_date'].iloc[0])
                target_dir_name = ref_date
            else:
                target_dir_name = file_id
        except Exception as e:
            print(f"âš ï¸ è¯»å– CSV å¤±è´¥: {csv_file.name}, ä½¿ç”¨æ–‡ä»¶IDä½œä¸ºç›®å½•å")
            target_dir_name = file_id
        
        # åˆ›å»ºç›®æ ‡ç›®å½•ç»“æ„
        target_dir = Path(PROJECT_ROOT) / UPDATE_DIR / target_dir_name
        target_pdf_dir = target_dir / "pdf"
        target_md_dir = target_dir / "md"
        target_csv_dir = target_dir / "csv"
        
        target_pdf_dir.mkdir(parents=True, exist_ok=True)
        target_md_dir.mkdir(parents=True, exist_ok=True)
        target_csv_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {file_id} -> {target_dir_name}")
        
        # å¤åˆ¶æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•
        pdf_file = pdf_dir / f"{file_id}.pdf"
        md_file = md_dir / f"{file_id}.md"
        
        if pdf_file.exists():
            target_pdf = target_pdf_dir / f"{file_id}.pdf"
            if not target_pdf.exists():
                shutil.copy2(pdf_file, target_pdf)
                print(f"  âœ“ å¤åˆ¶ PDF: {pdf_file.name}")
        
        if md_file.exists():
            target_md = target_md_dir / f"{file_id}.md"
            if not target_md.exists():
                shutil.copy2(md_file, target_md)
                print(f"  âœ“ å¤åˆ¶ MD: {md_file.name}")
        
        target_csv = target_csv_dir / f"{file_id}.csv"
        if not target_csv.exists():
            shutil.copy2(csv_file, target_csv)
            print(f"  âœ“ å¤åˆ¶ CSV: {csv_file.name}")
        
        # åˆå¹¶æ•°æ®åˆ°ä¸»æ–‡ä»¶
        print(f"  ğŸ“Š åˆå¹¶æ•°æ®åˆ°ä¸»æ–‡ä»¶...")
        try:
            merge_csv_to_main(target_csv, all_csv_path, covid_csv_path)
        except Exception as e:
            logging.error(f"åˆå¹¶å¤±è´¥: {e}")
            continue
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    temp_root = Path(PROJECT_ROOT) / UPDATE_DIR / "tmp"
    if temp_root.exists():
        shutil.rmtree(temp_root)
        print(f"\nğŸ§¹ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_root}")
    
    print(f"\nâœ… æ•°æ®æ•´ç†å’Œåˆå¹¶ä»»åŠ¡å®Œæˆ")


def merge_csv_to_main(new_csv, all_csv_path, covid_csv_path):
    """åˆå¹¶æ–°æ•°æ®åˆ°ä¸»æ•°æ®æ–‡ä»¶"""
    import pandas as pd
    
    # è¯»å–æ–°æ•°æ®
    new_df = pd.read_csv(new_csv, encoding='utf-8-sig')
    
    # åˆå¹¶åˆ°å…¨éƒ¨æ•°æ®æ–‡ä»¶
    if all_csv_path.exists():
        all_df = pd.read_csv(all_csv_path, encoding='utf-8-sig')
        combined_df = pd.concat([new_df, all_df], ignore_index=True)
        combined_df = combined_df.drop_duplicates(
            subset=['reference_date', 'target_end_date', 'pathogen'],
            keep='first'
        )
        combined_df['_sort_date'] = pd.to_datetime(combined_df['reference_date'], errors='coerce')
        combined_df = combined_df.sort_values(
            ['_sort_date', 'pathogen'], 
            ascending=[False, True]
        ).drop(columns=['_sort_date'])
        combined_df.to_csv(all_csv_path, index=False, encoding='utf-8-sig')
        print(f"    âœ“ æ›´æ–°å…¨éƒ¨æ•°æ®: {len(combined_df)} è¡Œ")
    else:
        new_df['_sort_date'] = pd.to_datetime(new_df['reference_date'], errors='coerce')
        new_df = new_df.sort_values(
            ['_sort_date', 'pathogen'], 
            ascending=[False, True]
        ).drop(columns=['_sort_date'])
        new_df.to_csv(all_csv_path, index=False, encoding='utf-8-sig')
        print(f"    âœ“ åˆ›å»ºå…¨éƒ¨æ•°æ®: {len(new_df)} è¡Œ")
    
    # åˆå¹¶åˆ°æ–°å† æ•°æ®æ–‡ä»¶
    covid_new_df = new_df[new_df['pathogen'].str.contains('æ–°å‹å† çŠ¶ç—…æ¯’', na=False)]
    
    if len(covid_new_df) > 0:
        if covid_csv_path.exists():
            covid_df = pd.read_csv(covid_csv_path, encoding='utf-8-sig')
            combined_covid_df = pd.concat([covid_new_df, covid_df], ignore_index=True)
            combined_covid_df = combined_covid_df.drop_duplicates(
                subset=['reference_date', 'target_end_date', 'pathogen'],
                keep='first'
            )
            combined_covid_df['_sort_date'] = pd.to_datetime(combined_covid_df['reference_date'], errors='coerce')
            combined_covid_df = combined_covid_df.sort_values(
                '_sort_date', 
                ascending=False
            ).drop(columns=['_sort_date'])
            combined_covid_df.to_csv(covid_csv_path, index=False, encoding='utf-8-sig')
            print(f"    âœ“ æ›´æ–°æ–°å† æ•°æ®: {len(combined_covid_df)} è¡Œ")
        else:
            covid_new_df['_sort_date'] = pd.to_datetime(covid_new_df['reference_date'], errors='coerce')
            covid_new_df = covid_new_df.sort_values(
                '_sort_date', 
                ascending=False
            ).drop(columns=['_sort_date'])
            covid_new_df.to_csv(covid_csv_path, index=False, encoding='utf-8-sig')
            print(f"    âœ“ åˆ›å»ºæ–°å† æ•°æ®: {len(covid_new_df)} è¡Œ")


def update_history_file(**context):
    """
    å°†å¤„ç†è¿‡çš„æ–°é“¾æ¥å†™å›å†å²è®°å½•æ–‡ä»¶
    """
    import sys
    sys.path.insert(0, PROJECT_ROOT)
    
    # è·å–æ–°é“¾æ¥
    new_links = context['task_instance'].xcom_pull(
        key='new_links', 
        task_ids='fetch_new_links'
    )
    
    if not new_links:
        print("âš ï¸  æ²¡æœ‰æ–°é“¾æ¥éœ€è¦å†™å…¥å†å²æ–‡ä»¶")
        return 0
    
    # å†™å…¥å†å²æ–‡ä»¶
    history_file_path = Path(PROJECT_ROOT) / HISTORY_URL_FILE
    
    try:
        append_links_to_file(str(history_file_path), new_links)
        print(f"âœ… å·²å°† {len(new_links)} ä¸ªæ–°é“¾æ¥å†™å…¥å†å²æ–‡ä»¶")
        return len(new_links)
    except Exception as e:
        logging.error(f"å†™å…¥å†å²æ–‡ä»¶å¤±è´¥: {e}")
        raise


def send_completion_notification(**context):
    """å‘é€å®Œæˆé€šçŸ¥"""
    import pandas as pd
    
    all_csv_path = Path(PROJECT_ROOT) / ALL_CSV
    covid_csv_path = Path(PROJECT_ROOT) / COVID_CSV
    
    # è·å–æ‰§è¡Œæ—¶é—´ï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬ï¼‰
    logical_date = context.get('logical_date') or context.get('execution_date')
    execution_time = logical_date.strftime('%Y-%m-%d %H:%M:%S') if logical_date else datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # è·å–æ–°é“¾æ¥æ•°é‡å’Œçˆ¬å–çŠ¶æ€
    new_links_count = context['task_instance'].xcom_pull(
        key='link_count', 
        task_ids='fetch_new_links'
    )
    
    crawl_failed = context['task_instance'].xcom_pull(
        key='crawl_failed', 
        task_ids='fetch_new_links'
    )
    
    # ç»Ÿè®¡æ•°æ®
    stats = {
        "execution_date": execution_time,
        "dag_run_id": context['dag_run'].run_id,
        "new_links_count": new_links_count or 0,
        "crawl_failed": crawl_failed or False,
    }
    
    if all_csv_path.exists():
        all_df = pd.read_csv(all_csv_path, encoding='utf-8-sig')
        stats['total_records'] = len(all_df)
        stats['latest_date'] = all_df['reference_date'].iloc[0] if len(all_df) > 0 else "N/A"
    
    if covid_csv_path.exists():
        covid_df = pd.read_csv(covid_csv_path, encoding='utf-8-sig')
        stats['covid_records'] = len(covid_df)
    
    print("\n" + "="*60)
    print("âœ… æ•°æ®æ›´æ–°ä»»åŠ¡å…¨éƒ¨å®Œæˆ!")
    print("="*60)
    print(f"ğŸ“… æ‰§è¡Œæ—¶é—´: {stats['execution_date']}")
    print(f"ğŸ†” è¿è¡ŒID: {stats['dag_run_id']}")
    
    # çˆ¬å–çŠ¶æ€æç¤º
    if stats['crawl_failed']:
        print(f"âš ï¸  ç½‘ç«™çˆ¬å–: å¤±è´¥ï¼ˆä½¿ç”¨å†å²æ•°æ®æˆ–å¤‡ç”¨æ–‡ä»¶ï¼‰")
    else:
        print(f"âœ… ç½‘ç«™çˆ¬å–: æˆåŠŸ")
    
    print(f"ğŸ”— å¤„ç†æ–°é“¾æ¥: {stats['new_links_count']} ä¸ª")
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {stats.get('total_records', 0)} è¡Œ")
    print(f"ğŸ¦  æ–°å† æ•°æ®é‡: {stats.get('covid_records', 0)} è¡Œ")
    print(f"ğŸ“† æœ€æ–°æ—¥æœŸ: {stats.get('latest_date', 'N/A')}")
    
    # æ·»åŠ å»ºè®®
    if stats['crawl_failed'] and stats['new_links_count'] == 0:
        print("\nğŸ’¡ å»ºè®®:")
        print("  - æ£€æŸ¥ CDC ç½‘ç«™æ˜¯å¦å¯è®¿é—®")
        print("  - æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("  - æˆ–åœ¨ config/url_surveillance_new.txt ä¸­æ‰‹åŠ¨æ·»åŠ é“¾æ¥")
    
    print("="*60)
    
    return stats


# åˆ›å»º DAG
with DAG(
    dag_id='cdc_covid19_data_update',
    default_args=DEFAULT_ARGS,
    description='è‡ªåŠ¨æ›´æ–°ä¸­å›½ç–¾æ§ä¸­å¿ƒCOVID-19ç›‘æµ‹æ•°æ®',
    schedule='0 0 * * 0',  # æ¯å‘¨æ—¥0ç‚¹æ‰§è¡Œ
    start_date=datetime(2025, 10, 1),
    catchup=False,
    tags=['cdc', 'covid19', 'data_pipeline'],
) as dag:
    
    # ä»»åŠ¡1: çˆ¬å–æ–°é“¾æ¥
    task_fetch_links = PythonOperator(
        task_id='fetch_new_links',
        python_callable=fetch_new_links_task,
    )
    
    # ä»»åŠ¡2: æ£€æŸ¥æ–°é“¾æ¥ï¼ˆä½¿ç”¨ ShortCircuitOperator å®ç°æ¡ä»¶ç»ˆæ­¢ï¼‰
    task_check_links = ShortCircuitOperator(
        task_id='check_new_links',
        python_callable=check_new_links,
    )
    
    # ä»»åŠ¡3: ä¸‹è½½ç½‘é¡µä¸º PDF
    task_download_pdf = PythonOperator(
        task_id='download_web_to_pdf',
        python_callable=download_web_to_pdf,
    )
    
    # ä»»åŠ¡4: è½¬æ¢ PDF ä¸º Markdown
    task_convert_md = PythonOperator(
        task_id='convert_pdf_to_md',
        python_callable=convert_pdf_to_md,
    )
    
    # ä»»åŠ¡5: ä» Markdown æå–æ•°æ®
    task_extract_data = PythonOperator(
        task_id='extract_data_from_md',
        python_callable=extract_data_from_md,
    )
    
    # ä»»åŠ¡6: æ•´ç†å’Œåˆå¹¶æ•°æ®
    task_merge_data = PythonOperator(
        task_id='organize_and_merge_data',
        python_callable=organize_and_merge_data,
    )
    
    # ä»»åŠ¡7: æ›´æ–°å†å²è®°å½•æ–‡ä»¶ï¼ˆåªåœ¨æ‰€æœ‰å‰ç½®ä»»åŠ¡æˆåŠŸæ—¶æ‰§è¡Œï¼‰
    task_update_history = PythonOperator(
        task_id='update_history_file',
        python_callable=update_history_file,
        trigger_rule='all_success',  # æ˜ç¡®æŒ‡å®šï¼šåªæœ‰æ‰€æœ‰ä¸Šæ¸¸ä»»åŠ¡æˆåŠŸæ‰æ‰§è¡Œ
    )
    
    # ä»»åŠ¡8: å‘é€å®Œæˆé€šçŸ¥ï¼ˆæ— è®ºæ˜¯å¦æœ‰æ–°é“¾æ¥éƒ½ä¼šæ‰§è¡Œï¼‰
    task_notify = PythonOperator(
        task_id='send_completion_notification',
        python_callable=send_completion_notification,
        trigger_rule='all_done',  # æ— è®ºä¸Šæ¸¸ä»»åŠ¡æˆåŠŸã€å¤±è´¥æˆ–è·³è¿‡ï¼Œéƒ½ä¼šæ‰§è¡Œ
    )
    
    # ä»»åŠ¡9: è‡ªåŠ¨æ¨é€åˆ° GitHubï¼ˆåªåœ¨æœ‰æ•°æ®æ›´æ–°æ—¶æ‰§è¡Œï¼‰
    task_git_push = BashOperator(
        task_id='git_push_to_github',
        bash_command=f"""
        cd {PROJECT_ROOT} && \
        git add data/ update/ config/url_surveillance_history.txt && \
        git diff --cached --quiet || (git commit -m "è‡ªåŠ¨æ›´æ–°: $(date '+%Y-%m-%d %H:%M:%S') CDCç›‘æµ‹æ•°æ®" && git push)
        """,
        trigger_rule='all_success',  # åªæœ‰æ‰€æœ‰ä¸Šæ¸¸ä»»åŠ¡æˆåŠŸæ‰æ‰§è¡Œ push
    )
    
    # å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
    # ä¸»æµç¨‹ï¼šçˆ¬å– -> æ£€æŸ¥ -> ä¸‹è½½ -> è½¬æ¢ -> æå– -> åˆå¹¶ -> æ›´æ–°å†å²
    task_fetch_links >> task_check_links >> task_download_pdf >> task_convert_md >> task_extract_data >> task_merge_data >> task_update_history
    
    # é€šçŸ¥ä»»åŠ¡ï¼šæ— è®ºæ•°æ®å¤„ç†æ˜¯å¦æˆåŠŸéƒ½ä¼šæ‰§è¡Œ
    [task_fetch_links, task_update_history] >> task_notify
    
    # Git Push ä»»åŠ¡ï¼šåªæœ‰æ•°æ®å¤„ç†æˆåŠŸåæ‰æ‰§è¡Œ
    task_update_history >> task_git_push

